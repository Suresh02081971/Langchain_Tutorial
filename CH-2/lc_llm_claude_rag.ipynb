{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed31073",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RAG (Retrieval Augmented Generation) with Claude\n",
    "Install required packages:\n",
    "pip install langchain langchain-anthropic langchain-community faiss-cpu sentence-transformers\n",
    "\"\"\"\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "import os\n",
    "\n",
    "# Set your API key\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "# Initialize Claude\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-sonnet-4-5-20250929\",\n",
    "    temperature=0.3,  # Lower temperature for factual retrieval\n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# Example 1: Basic RAG with Simple Documents\n",
    "# ============================================================\n",
    "print(\"Example 1: Basic RAG Setup\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"Python is a high-level programming language known for its simplicity and readability.\",\n",
    "    \"Machine learning is a subset of AI that enables systems to learn from data.\",\n",
    "    \"LangChain is a framework for developing applications powered by language models.\",\n",
    "    \"Claude is an AI assistant created by Anthropic that can help with various tasks.\",\n",
    "    \"RAG combines retrieval systems with language models to provide accurate answers.\"\n",
    "]\n",
    "\n",
    "# Convert to Document objects\n",
    "docs = [Document(page_content=doc) for doc in documents]\n",
    "\n",
    "# Create embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create vector store\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# Create retrieval chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "# Ask questions\n",
    "questions = [\n",
    "    \"What is Python?\",\n",
    "    \"Tell me about Claude\",\n",
    "    \"What is RAG?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    result = qa_chain.invoke(question)\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {result['result']}\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# Example 2: RAG with Document Splitting\n",
    "# ============================================================\n",
    "print(\"Example 2: Processing Longer Documents\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Longer document\n",
    "long_text = \"\"\"\n",
    "Python was created by Guido van Rossum and first released in 1991. \n",
    "It emphasizes code readability with its use of significant indentation.\n",
    "\n",
    "Python supports multiple programming paradigms including structured, \n",
    "object-oriented, and functional programming. It has a comprehensive \n",
    "standard library, often described as having \"batteries included.\"\n",
    "\n",
    "Popular frameworks include Django for web development, NumPy for \n",
    "numerical computing, and TensorFlow for machine learning. Python is \n",
    "widely used in data science, artificial intelligence, web development,\n",
    "automation, and scientific computing.\n",
    "\"\"\"\n",
    "\n",
    "# Split the text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "chunks = text_splitter.create_documents([long_text])\n",
    "print(f\"Split document into {len(chunks)} chunks\")\n",
    "\n",
    "# Create vector store\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "# Query\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "result = qa_chain.invoke(\"What frameworks does Python have?\")\n",
    "print(f\"\\nQ: What frameworks does Python have?\")\n",
    "print(f\"A: {result['result']}\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# Example 3: RAG with Custom Prompts\n",
    "# ============================================================\n",
    "print(\"Example 3: Custom RAG Prompt\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Create custom prompt template\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question. \n",
    "If you don't know the answer, just say you don't know. Don't make up an answer.\n",
    "Keep the answer concise and cite which part of the context you used.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create chain with custom prompt\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "\n",
    "result = qa_chain.invoke(\"Who created Python?\")\n",
    "print(f\"Q: Who created Python?\")\n",
    "print(f\"A: {result['result']}\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# Example 4: RAG with Multiple Document Sources\n",
    "# ============================================================\n",
    "print(\"Example 4: Multiple Document Sources\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Different document sources\n",
    "company_docs = [\n",
    "    Document(\n",
    "        page_content=\"Our company was founded in 2020 and specializes in AI solutions.\",\n",
    "        metadata={\"source\": \"about.txt\", \"type\": \"company_info\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"We offer 24/7 customer support via email and phone.\",\n",
    "        metadata={\"source\": \"support.txt\", \"type\": \"support_info\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Our main product is an AI-powered analytics platform.\",\n",
    "        metadata={\"source\": \"products.txt\", \"type\": \"product_info\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create vector store with metadata\n",
    "vectorstore = FAISS.from_documents(company_docs, embeddings)\n",
    "\n",
    "# Retrieve with source information\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "result = qa_chain.invoke(\"What does the company do?\")\n",
    "print(f\"Q: What does the company do?\")\n",
    "print(f\"A: {result['result']}\")\n",
    "print(f\"\\nSources used:\")\n",
    "for doc in result['source_documents']:\n",
    "    print(f\"  - {doc.metadata['source']}: {doc.metadata['type']}\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# Example 5: RAG with Similarity Search\n",
    "# ============================================================\n",
    "print(\"Example 5: Direct Similarity Search\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Create a knowledge base\n",
    "knowledge_base = [\n",
    "    \"The capital of France is Paris.\",\n",
    "    \"The capital of Germany is Berlin.\",\n",
    "    \"The capital of Italy is Rome.\",\n",
    "    \"The capital of Spain is Madrid.\",\n",
    "    \"The capital of Portugal is Lisbon.\"\n",
    "]\n",
    "\n",
    "docs = [Document(page_content=text) for text in knowledge_base]\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# Perform similarity search\n",
    "query = \"What is the capital of France?\"\n",
    "relevant_docs = vectorstore.similarity_search(query, k=2)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(\"Most relevant documents:\")\n",
    "for i, doc in enumerate(relevant_docs, 1):\n",
    "    print(f\"{i}. {doc.page_content}\")\n",
    "\n",
    "# Use with Claude\n",
    "context = \"\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "response = llm.invoke(f\"Based on this context: {context}\\n\\nQuestion: {query}\")\n",
    "print(f\"\\nClaude's answer: {response.content}\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# Example 6: RAG with File Loading\n",
    "# ============================================================\n",
    "print(\"Example 6: Loading from Text Files\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Simulate loading from files (in practice, use actual files)\n",
    "file_contents = {\n",
    "    \"python_basics.txt\": \"\"\"\n",
    "    Python is an interpreted, high-level programming language.\n",
    "    It uses dynamic typing and garbage collection.\n",
    "    Python's design philosophy emphasizes code readability.\n",
    "    \"\"\",\n",
    "    \"python_libraries.txt\": \"\"\"\n",
    "    Popular Python libraries include:\n",
    "    - NumPy for numerical computing\n",
    "    - Pandas for data manipulation\n",
    "    - Matplotlib for visualization\n",
    "    - Scikit-learn for machine learning\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "# Create documents with metadata\n",
    "file_docs = []\n",
    "for filename, content in file_contents.items():\n",
    "    file_docs.append(\n",
    "        Document(\n",
    "            page_content=content,\n",
    "            metadata={\"source\": filename}\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Split and create vector store\n",
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "split_docs = text_splitter.split_documents(file_docs)\n",
    "\n",
    "vectorstore = FAISS.from_documents(split_docs, embeddings)\n",
    "\n",
    "# Query with source tracking\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "result = qa_chain.invoke(\"What Python libraries are mentioned?\")\n",
    "print(f\"Q: What Python libraries are mentioned?\")\n",
    "print(f\"A: {result['result']}\")\n",
    "print(f\"Source: {result['source_documents'][0].metadata['source']}\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# Example 7: Advanced RAG with MMR (Maximum Marginal Relevance)\n",
    "# ============================================================\n",
    "print(\"Example 7: Using MMR for Diverse Results\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "tech_docs = [\n",
    "    \"Python is great for web development with frameworks like Django and Flask.\",\n",
    "    \"Python excels in data science with libraries like Pandas and NumPy.\",\n",
    "    \"Python is used in machine learning with TensorFlow and PyTorch.\",\n",
    "    \"JavaScript is the language of the web and runs in browsers.\",\n",
    "    \"Java is widely used for enterprise applications and Android development.\",\n",
    "]\n",
    "\n",
    "docs = [Document(page_content=text) for text in tech_docs]\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# Use MMR to get diverse results\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 3, \"fetch_k\": 5}\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "result = qa_chain.invoke(\"What is Python used for?\")\n",
    "print(f\"Q: What is Python used for?\")\n",
    "print(f\"A: {result['result']}\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# Example 8: RAG with Conversation Memory\n",
    "# ============================================================\n",
    "print(\"Example 8: Conversational RAG\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Create memory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    output_key=\"answer\"\n",
    ")\n",
    "\n",
    "# Create conversational chain\n",
    "conv_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    memory=memory,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Have a conversation\n",
    "questions = [\n",
    "    \"What programming languages are mentioned?\",\n",
    "    \"Which one is best for web development?\",\n",
    "    \"What about mobile development?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    result = conv_chain.invoke({\"question\": question})\n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"A: {result['answer']}\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# Example 9: Evaluating RAG Results\n",
    "# ============================================================\n",
    "print(\"Example 9: Checking Retrieved Context\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "query = \"What is machine learning?\"\n",
    "retrieved_docs = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents:\")\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"\\n{i}. {doc.page_content}\")\n",
    "\n",
    "# Show relevance scores\n",
    "docs_with_scores = vectorstore.similarity_search_with_score(query, k=3)\n",
    "print(\"\\nWith relevance scores:\")\n",
    "for doc, score in docs_with_scores:\n",
    "    print(f\"Score: {score:.4f} - {doc.page_content[:50]}...\")\n",
    "\n",
    "# ============================================================\n",
    "# Example 10: Complete RAG Application\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Example 10: Complete RAG Application\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class DocumentQA:\n",
    "    \"\"\"Complete RAG application class\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key):\n",
    "        os.environ[\"ANTHROPIC_API_KEY\"] = api_key\n",
    "        self.llm = ChatAnthropic(\n",
    "            model=\"claude-sonnet-4-5-20250929\",\n",
    "            temperature=0.3\n",
    "        )\n",
    "        self.embeddings = HuggingFaceEmbeddings()\n",
    "        self.vectorstore = None\n",
    "    \n",
    "    def add_documents(self, texts, metadata_list=None):\n",
    "        \"\"\"Add documents to the knowledge base\"\"\"\n",
    "        if metadata_list:\n",
    "            docs = [\n",
    "                Document(page_content=text, metadata=meta)\n",
    "                for text, meta in zip(texts, metadata_list)\n",
    "            ]\n",
    "        else:\n",
    "            docs = [Document(page_content=text) for text in texts]\n",
    "        \n",
    "        if self.vectorstore is None:\n",
    "            self.vectorstore = FAISS.from_documents(docs, self.embeddings)\n",
    "        else:\n",
    "            self.vectorstore.add_documents(docs)\n",
    "        \n",
    "        print(f\"Added {len(docs)} documents to knowledge base\")\n",
    "    \n",
    "    def query(self, question, return_sources=False):\n",
    "        \"\"\"Query the knowledge base\"\"\"\n",
    "        if self.vectorstore is None:\n",
    "            return \"No documents in knowledge base\"\n",
    "        \n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            retriever=self.vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "            return_source_documents=return_sources\n",
    "        )\n",
    "        \n",
    "        result = qa_chain.invoke(question)\n",
    "        \n",
    "        if return_sources:\n",
    "            return {\n",
    "                \"answer\": result['result'],\n",
    "                \"sources\": [doc.page_content for doc in result['source_documents']]\n",
    "            }\n",
    "        return result['result']\n",
    "    \n",
    "    def save_index(self, path):\n",
    "        \"\"\"Save the vector store to disk\"\"\"\n",
    "        if self.vectorstore:\n",
    "            self.vectorstore.save_local(path)\n",
    "            print(f\"Index saved to {path}\")\n",
    "    \n",
    "    def load_index(self, path):\n",
    "        \"\"\"Load a vector store from disk\"\"\"\n",
    "        self.vectorstore = FAISS.load_local(\n",
    "            path,\n",
    "            self.embeddings,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "        print(f\"Index loaded from {path}\")\n",
    "\n",
    "# Use the application\n",
    "qa_app = DocumentQA(\"your-api-key-here\")\n",
    "\n",
    "# Add documents\n",
    "qa_app.add_documents([\n",
    "    \"Artificial Intelligence is the simulation of human intelligence by machines.\",\n",
    "    \"Machine Learning is a subset of AI that learns from data.\",\n",
    "    \"Deep Learning uses neural networks with multiple layers.\",\n",
    "    \"Natural Language Processing enables computers to understand human language.\"\n",
    "])\n",
    "\n",
    "# Query\n",
    "answer = qa_app.query(\"What is the difference between AI and ML?\")\n",
    "print(f\"\\nQuestion: What is the difference between AI and ML?\")\n",
    "print(f\"Answer: {answer}\")\n",
    "\n",
    "# Query with sources\n",
    "result = qa_app.query(\"What is deep learning?\", return_sources=True)\n",
    "print(f\"\\nQuestion: What is deep learning?\")\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "print(f\"Sources: {len(result['sources'])} documents retrieved\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RAG setup complete! Try running these examples.\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
